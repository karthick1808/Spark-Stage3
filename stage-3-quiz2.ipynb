{"cells":[{"cell_type":"code","source":["from pyspark import SparkConf, SparkContext\nconf = SparkConf().setAppName(\"Quiz\")\nsc = SparkContext.getOrCreate(conf=conf)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2bb1599a-eb8c-44b5-9b15-341c4ea8f0a4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rdd = sc.textFile('/FileStore/tables/filter.txt')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2abb0b2f-0052-4e27-9dd3-fbe5a026247c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rdd.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de5cde7b-5dc8-4ab4-a6d3-530de943b356"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[3]: ['this mango company animal',\n 'cat dog ant mic laptop',\n 'chair switch mobile am charger cover',\n 'amanda any alarm ant']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[3]: ['this mango company animal',\n 'cat dog ant mic laptop',\n 'chair switch mobile am charger cover',\n 'amanda any alarm ant']"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["rdd2 = rdd.flatMap(lambda x: x.split(' '))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae50ac26-0cd7-44b5-b448-a7e4fc9d9ce4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rdd2.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8dfef4d-a785-4d21-9f6c-dc4ecef7babe"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[5]: ['this',\n 'mango',\n 'company',\n 'animal',\n 'cat',\n 'dog',\n 'ant',\n 'mic',\n 'laptop',\n 'chair',\n 'switch',\n 'mobile',\n 'am',\n 'charger',\n 'cover',\n 'amanda',\n 'any',\n 'alarm',\n 'ant']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[5]: ['this',\n 'mango',\n 'company',\n 'animal',\n 'cat',\n 'dog',\n 'ant',\n 'mic',\n 'laptop',\n 'chair',\n 'switch',\n 'mobile',\n 'am',\n 'charger',\n 'cover',\n 'amanda',\n 'any',\n 'alarm',\n 'ant']"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["rdd3 = rdd2.filter(lambda x: len(x) != 0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"65b47bc2-3024-40ce-813a-705230931c4f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rdd3.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46f8c948-b1df-4811-aba0-20312341cf8c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[7]: ['this',\n 'mango',\n 'company',\n 'animal',\n 'cat',\n 'dog',\n 'ant',\n 'mic',\n 'laptop',\n 'chair',\n 'switch',\n 'mobile',\n 'am',\n 'charger',\n 'cover',\n 'amanda',\n 'any',\n 'alarm',\n 'ant']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[7]: ['this',\n 'mango',\n 'company',\n 'animal',\n 'cat',\n 'dog',\n 'ant',\n 'mic',\n 'laptop',\n 'chair',\n 'switch',\n 'mobile',\n 'am',\n 'charger',\n 'cover',\n 'amanda',\n 'any',\n 'alarm',\n 'ant']"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["rdd4 = rdd3.map(lambda x: (x,1))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b10c006-0da1-42a6-93d0-f589208e91ef"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rdd4.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf997f80-a1f6-467f-bbd5-8c6c660ec88d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[9]: [('this', 1),\n ('mango', 1),\n ('company', 1),\n ('animal', 1),\n ('cat', 1),\n ('dog', 1),\n ('ant', 1),\n ('mic', 1),\n ('laptop', 1),\n ('chair', 1),\n ('switch', 1),\n ('mobile', 1),\n ('am', 1),\n ('charger', 1),\n ('cover', 1),\n ('amanda', 1),\n ('any', 1),\n ('alarm', 1),\n ('ant', 1)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[9]: [('this', 1),\n ('mango', 1),\n ('company', 1),\n ('animal', 1),\n ('cat', 1),\n ('dog', 1),\n ('ant', 1),\n ('mic', 1),\n ('laptop', 1),\n ('chair', 1),\n ('switch', 1),\n ('mobile', 1),\n ('am', 1),\n ('charger', 1),\n ('cover', 1),\n ('amanda', 1),\n ('any', 1),\n ('alarm', 1),\n ('ant', 1)]"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["rdd4.reduceByKey(lambda x,y : x+y).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b16ab7c6-6f6b-4ace-8b4c-361c104fb007"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[10]: [('this', 1),\n ('mango', 1),\n ('cat', 1),\n ('ant', 2),\n ('laptop', 1),\n ('chair', 1),\n ('switch', 1),\n ('mobile', 1),\n ('am', 1),\n ('company', 1),\n ('animal', 1),\n ('dog', 1),\n ('mic', 1),\n ('charger', 1),\n ('cover', 1),\n ('amanda', 1),\n ('any', 1),\n ('alarm', 1)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[10]: [('this', 1),\n ('mango', 1),\n ('cat', 1),\n ('ant', 2),\n ('laptop', 1),\n ('chair', 1),\n ('switch', 1),\n ('mobile', 1),\n ('am', 1),\n ('company', 1),\n ('animal', 1),\n ('dog', 1),\n ('mic', 1),\n ('charger', 1),\n ('cover', 1),\n ('amanda', 1),\n ('any', 1),\n ('alarm', 1)]"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["singlecom = rdd.flatMap(lambda x : x.split(' ')).map(lambda x: (x,1)).reduceByKey(lambda x,y : x+y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c087a0b-e6c4-49ab-ad7e-6c3de8020be4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["singlecom.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79e8090c-b08b-4b97-8322-c4c22038e643"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[12]: [('this', 1),\n ('mango', 1),\n ('cat', 1),\n ('ant', 2),\n ('laptop', 1),\n ('chair', 1),\n ('switch', 1),\n ('mobile', 1),\n ('am', 1),\n ('company', 1),\n ('animal', 1),\n ('dog', 1),\n ('mic', 1),\n ('charger', 1),\n ('cover', 1),\n ('amanda', 1),\n ('any', 1),\n ('alarm', 1)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[12]: [('this', 1),\n ('mango', 1),\n ('cat', 1),\n ('ant', 2),\n ('laptop', 1),\n ('chair', 1),\n ('switch', 1),\n ('mobile', 1),\n ('am', 1),\n ('company', 1),\n ('animal', 1),\n ('dog', 1),\n ('mic', 1),\n ('charger', 1),\n ('cover', 1),\n ('amanda', 1),\n ('any', 1),\n ('alarm', 1)]"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["movie = sc.textFile(\"/FileStore/tables/movie_ratings.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78330e7b-7d75-4206-8530-721172799d45"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["movie.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b9ad252-4d42-45cf-af71-5567eba3b681"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[14]: ['The Shawshank Redemption,3',\n 'The Matrix,5',\n '12 Angry Men,3',\n '12 Angry Men,4',\n 'The Matrix,5',\n 'Pulp Fiction,4',\n 'The Godfather,5',\n 'The Shawshank Redemption,2',\n 'Pulp Fiction,4',\n 'The Godfather,5',\n '12 Angry Men,2',\n 'The Godfather,3',\n 'Pulp Fiction,4',\n '12 Angry Men,1',\n 'The Shawshank Redemption,2',\n '12 Angry Men,1',\n 'The Shawshank Redemption,5',\n 'Pulp Fiction,5',\n 'Pulp Fiction,2',\n 'The Matrix,4']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[14]: ['The Shawshank Redemption,3',\n 'The Matrix,5',\n '12 Angry Men,3',\n '12 Angry Men,4',\n 'The Matrix,5',\n 'Pulp Fiction,4',\n 'The Godfather,5',\n 'The Shawshank Redemption,2',\n 'Pulp Fiction,4',\n 'The Godfather,5',\n '12 Angry Men,2',\n 'The Godfather,3',\n 'Pulp Fiction,4',\n '12 Angry Men,1',\n 'The Shawshank Redemption,2',\n '12 Angry Men,1',\n 'The Shawshank Redemption,5',\n 'Pulp Fiction,5',\n 'Pulp Fiction,2',\n 'The Matrix,4']"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["moviedata = movie.map(lambda x :(\"(\" + x + \")\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3d81a86-508a-437a-b579-03a7eee3cbdb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["moviedata.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"076a14ca-fe20-45dc-bf0d-8f17896552cc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[34]: ['(The Shawshank Redemption,3)',\n '(The Matrix,5)',\n '(12 Angry Men,3)',\n '(12 Angry Men,4)',\n '(The Matrix,5)',\n '(Pulp Fiction,4)',\n '(The Godfather,5)',\n '(The Shawshank Redemption,2)',\n '(Pulp Fiction,4)',\n '(The Godfather,5)',\n '(12 Angry Men,2)',\n '(The Godfather,3)',\n '(Pulp Fiction,4)',\n '(12 Angry Men,1)',\n '(The Shawshank Redemption,2)',\n '(12 Angry Men,1)',\n '(The Shawshank Redemption,5)',\n '(Pulp Fiction,5)',\n '(Pulp Fiction,2)',\n '(The Matrix,4)']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[34]: ['(The Shawshank Redemption,3)',\n '(The Matrix,5)',\n '(12 Angry Men,3)',\n '(12 Angry Men,4)',\n '(The Matrix,5)',\n '(Pulp Fiction,4)',\n '(The Godfather,5)',\n '(The Shawshank Redemption,2)',\n '(Pulp Fiction,4)',\n '(The Godfather,5)',\n '(12 Angry Men,2)',\n '(The Godfather,3)',\n '(Pulp Fiction,4)',\n '(12 Angry Men,1)',\n '(The Shawshank Redemption,2)',\n '(12 Angry Men,1)',\n '(The Shawshank Redemption,5)',\n '(Pulp Fiction,5)',\n '(Pulp Fiction,2)',\n '(The Matrix,4)']"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["moviedata.reduceByKey(lambda x,y : x+y).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6e46556-ebe4-44c1-aa1c-8adc8a3fb324"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-2143154538714928>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmoviedata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreduceByKey\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0my\u001B[0m \u001B[0;34m:\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mcollect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    965\u001B[0m         \u001B[0;31m# Default path used in OSS Spark / for non-credential passthrough clusters:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    966\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mcss\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 967\u001B[0;31m             \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAndServe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    968\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    969\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 70.0 failed 1 times, most recent failure: Lost task 0.0 in stage 70.0 (TID 204) (ip-10-172-215-117.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: 'ValueError: too many values to unpack (expected 2)'. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 741, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 731, in process\n    out_iter = func(split_index, iterator)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 419, in func\n    return f(iterator)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 2181, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/databricks/spark/python/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\nValueError: too many values to unpack (expected 2)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:641)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:781)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:763)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:594)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:813)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1605)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:816)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:672)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2828)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2775)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2769)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2769)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1305)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1305)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1305)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3036)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2977)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2965)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1067)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2476)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1036)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1034)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:260)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor469.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: 'ValueError: too many values to unpack (expected 2)'. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 741, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 731, in process\n    out_iter = func(split_index, iterator)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 419, in func\n    return f(iterator)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 2181, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/databricks/spark/python/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\nValueError: too many values to unpack (expected 2)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:641)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:781)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:763)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:594)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:813)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1605)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:816)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:672)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n","errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 70.0 failed 1 times, most recent failure: Lost task 0.0 in stage 70.0 (TID 204) (ip-10-172-215-117.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: 'ValueError: too many values to unpack (expected 2)'. Full traceback below:","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-2143154538714928>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmoviedata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreduceByKey\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0my\u001B[0m \u001B[0;34m:\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mcollect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    965\u001B[0m         \u001B[0;31m# Default path used in OSS Spark / for non-credential passthrough clusters:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    966\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mcss\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 967\u001B[0;31m             \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAndServe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    968\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    969\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 70.0 failed 1 times, most recent failure: Lost task 0.0 in stage 70.0 (TID 204) (ip-10-172-215-117.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: 'ValueError: too many values to unpack (expected 2)'. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 741, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 731, in process\n    out_iter = func(split_index, iterator)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 419, in func\n    return f(iterator)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 2181, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/databricks/spark/python/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\nValueError: too many values to unpack (expected 2)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:641)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:781)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:763)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:594)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:813)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1605)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:816)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:672)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2828)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2775)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2769)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2769)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1305)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1305)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1305)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3036)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2977)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2965)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1067)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2476)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1036)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1034)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:260)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor469.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: 'ValueError: too many values to unpack (expected 2)'. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 741, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 731, in process\n    out_iter = func(split_index, iterator)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 419, in func\n    return f(iterator)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 2181, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/databricks/spark/python/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\nValueError: too many values to unpack (expected 2)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:641)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:781)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:763)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:594)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:813)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1605)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:816)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:672)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ab15e9a-e07a-4ae0-be6e-2c71cd673abd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"stage-3-quiz2","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2143154538714911}},"nbformat":4,"nbformat_minor":0}
